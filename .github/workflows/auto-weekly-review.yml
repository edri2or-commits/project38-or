name: Auto Weekly Review

on:
  schedule:
    # Every Monday at 09:00 UTC
    - cron: '0 9 * * 1'
  workflow_dispatch:
    inputs:
      dry_run:
        description: 'Dry run (no issues created)'
        required: false
        default: 'false'
        type: boolean

permissions:
  contents: write
  issues: write
  pull-requests: write

concurrency:
  group: weekly-review
  cancel-in-progress: true

jobs:
  review:
    name: Review Research Notes
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e .
          pip install -r requirements.txt

      - name: Run Weekly Review
        id: review
        env:
          DRY_RUN: ${{ github.event.inputs.dry_run || 'false' }}
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          echo "=== ADR-009 Auto Weekly Review ==="
          echo ""

          python -c "
          import os
          import json
          import subprocess
          from pathlib import Path
          from datetime import datetime

          # Import research module
          from src.research import (
              Classification,
              auto_classify,
              create_experiment_skeleton,
              find_unclassified_notes,
              parse_research_note,
              update_note_with_classification,
          )

          DRY_RUN = os.environ.get('DRY_RUN', 'false').lower() == 'true'
          notes_dir = Path('docs/research/notes')
          experiments_dir = Path('experiments')

          print(f'Mode: {\"DRY RUN\" if DRY_RUN else \"LIVE\"}')
          print(f'Notes directory: {notes_dir}')
          print()

          # Find unclassified notes
          unclassified = find_unclassified_notes(notes_dir)
          print(f'Found {len(unclassified)} unclassified notes')
          print()

          results = {
              'total_notes': len(unclassified),
              'classifications': {},
              'issues_created': [],
              'experiments_created': [],
          }

          for note_path in unclassified:
              print(f'Processing: {note_path.name}')

              # Parse note
              content = note_path.read_text()
              note = parse_research_note(content, note_path)

              # Classify
              classification, reason = auto_classify(note)
              print(f'  Classification: {classification.value}')
              print(f'  Reason: {reason}')

              # Track classification
              if classification.value not in results['classifications']:
                  results['classifications'][classification.value] = []
              results['classifications'][classification.value].append(note_path.name)

              if DRY_RUN:
                  print('  [DRY RUN] Would update note and create issue')
                  continue

              # Create GitHub Issue for Spikes
              issue_number = None
              if classification == Classification.SPIKE:
                  # Create issue
                  issue_title = f'[Spike] {note.title}'
                  issue_body = f'''## Research Note: {note.title}

          **Classification:** Spike
          **Reason:** {reason}

          ### Hypothesis
          > {note.hypothesis or 'To be determined'}

          ### Impact
          - **Scope:** {note.impact.scope.value if note.impact.scope else 'Unknown'}
          - **Effort:** {note.impact.effort.value if note.impact.effort else 'Unknown'}
          - **Risk:** {note.impact.risk.value if note.impact.risk else 'Unknown'}

          ### Next Steps
          - [ ] Review research note
          - [ ] Run experiment
          - [ ] Make ADOPT/REJECT decision

          ---
          Auto-generated by ADR-009 Weekly Review
          Research Note: `{note_path}`
          '''

                  # Create issue via gh CLI
                  result = subprocess.run([
                      'gh', 'issue', 'create',
                      '--title', issue_title,
                      '--body', issue_body,
                      '--label', 'research,spike',
                  ], capture_output=True, text=True)

                  if result.returncode == 0:
                      # Extract issue number from output
                      issue_url = result.stdout.strip()
                      issue_number = int(issue_url.split('/')[-1])
                      print(f'  Created Issue #{issue_number}')
                      results['issues_created'].append(issue_number)
                  else:
                      print(f'  Failed to create issue: {result.stderr}')

              # Create experiment skeleton for Spikes
              experiment_id = None
              if classification == Classification.SPIKE:
                  exp_dir = create_experiment_skeleton(note_path, experiments_dir)
                  if exp_dir:
                      experiment_id = exp_dir.name
                      print(f'  Created experiment: {experiment_id}')
                      results['experiments_created'].append(experiment_id)

              # Update note with classification
              update_note_with_classification(
                  note_path,
                  classification,
                  reason,
                  issue_number=issue_number,
                  experiment_id=experiment_id,
              )
              print(f'  Updated note')
              print()

          # Summary
          print('=== Summary ===')
          print(f'Total notes processed: {results[\"total_notes\"]}')
          for cls, notes in results['classifications'].items():
              print(f'  {cls}: {len(notes)}')
          print(f'Issues created: {len(results[\"issues_created\"])}')
          print(f'Experiments created: {len(results[\"experiments_created\"])}')

          # Set output
          with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
              f.write(f'total={results[\"total_notes\"]}\\n')
              f.write(f'spikes={len(results[\"classifications\"].get(\"Spike\", []))}\\n')
              f.write(f'issues={len(results[\"issues_created\"])}\\n')
          "

      - name: Commit changes
        if: ${{ github.event.inputs.dry_run != 'true' }}
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"

          # Check for changes
          if git diff --quiet && git diff --cached --quiet; then
            echo "No changes to commit"
            exit 0
          fi

          git add docs/research/notes/
          git add experiments/

          git commit -m "docs(research): auto weekly review $(date +%Y-%m-%d)

          - Classified unclassified research notes
          - Created issues for Spikes
          - Generated experiment skeletons

          Auto-generated by ADR-009 Weekly Review workflow"

          git push

      - name: Post summary
        uses: actions/github-script@v7
        with:
          script: |
            const total = '${{ steps.review.outputs.total }}';
            const spikes = '${{ steps.review.outputs.spikes }}';
            const issues = '${{ steps.review.outputs.issues }}';
            const dryRun = '${{ github.event.inputs.dry_run }}' === 'true';

            const summary = `## Weekly Review Summary

            | Metric | Count |
            |--------|-------|
            | Notes Processed | ${total} |
            | Spikes Identified | ${spikes} |
            | Issues Created | ${issues} |

            ${dryRun ? '⚠️ **DRY RUN** - No changes were made' : '✅ Changes committed'}
            `;

            await core.summary
              .addRaw(summary)
              .write();
